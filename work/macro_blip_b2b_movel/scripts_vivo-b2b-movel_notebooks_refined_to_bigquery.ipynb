{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from google.cloud import storage, bigquery\n",
    "from collections import namedtuple\n",
    "from functools import reduce\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "CURRENT_DATE_ARG = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_PROJECT = \"cloud-macro-blip\"\n",
    "BUCKET_NAME = \"vivo-api-manager-gcs-sp\"\n",
    "ORIGIN = \"vivo-b2b-movel\"\n",
    "TABLE_PREFIX = \"vivo_b2b_movel\"\n",
    "CURRENT_DATE = datetime.strptime(CURRENT_DATE_ARG, '%Y-%m-%dT%H:%M:%S') if CURRENT_DATE_ARG is not None else datetime.today() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRAZILIAN_TIMEDIFF = timedelta(hours=3)\n",
    "CURRENT_DATE = CURRENT_DATE - BRAZILIAN_TIMEDIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_midnight_hour(hour):\n",
    "  return hour >= 0 and hour <= 4\n",
    "\n",
    "if is_midnight_hour(CURRENT_DATE.hour):\n",
    "  CURRENT_DATE = CURRENT_DATE - timedelta(days=1)\n",
    "  CURRENT_DATE = CURRENT_DATE.replace(hour=23, minute=59, second=59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"refined_to_bigquery\")\\\n",
    "  .config('parentProject', PARENT_PROJECT)\\\n",
    "  .config(\"spark.sql.caseSensitive\", \"True\")\\\n",
    "  .config('spark.sql.session.timeZone', 'America/Sao_Paulo')\\\n",
    "  .config(\"spark.jars\", \"gs://spark-lib/bigquery/spark-3.3-bigquery-0.34.0.jar\")\\\n",
    "  .getOrCreate()\n",
    "\n",
    "gcsClient = storage.Client()\n",
    "bqClient = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path_in_refined_zone(datetime: datetime, origin: str, project_type: str, table_name: str):\n",
    "  return f\"gs://{BUCKET_NAME}/{origin}/refined-zone/data/{project_type}/{datetime.strftime('%Y%m')}/{table_name}.parquet\"\n",
    "\n",
    "# def get_funnel_path_in_refined_zone(datetime: datetime, origin: str, project_type: str, table_name: str):\n",
    "#   return f\"gs://{BUCKET_NAME}/{origin}/refined-zone/funnel/{project_type}/{datetime.strftime('%Y%m')}/{table_name}.parquet\"\n",
    "\n",
    "# def get_funnel_prefix_in_refined_zone(datetime: datetime, origin: str, project_type: str):\n",
    "#   return f\"{origin}/refined-zone/funnel/{project_type}/{datetime.strftime('%Y%m')}\"\n",
    "\n",
    "def get_data_prefix_in_refined_zone(datetime: datetime, origin: str, project_type: str):\n",
    "  return f\"{origin}/refined-zone/data/{project_type}/{datetime.strftime('%Y%m')}\"\n",
    "\n",
    "def reduce_dataframe(dfs: list):\n",
    "  if len(dfs) == 0:\n",
    "    return spark.createDataFrame([], StringType())\n",
    "  elif len(dfs) == 1:\n",
    "    return dfs[0]\n",
    "  else:\n",
    "    return reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), dfs)\n",
    "  \n",
    "def get_tmp_table_name(table_name: str):\n",
    "  return f'{TABLE_PREFIX}.{table_name}_tmp'\n",
    "\n",
    "def get_full_table_name(table_name: str):\n",
    "  return f'{TABLE_PREFIX}.{table_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = gcsClient.get_bucket(BUCKET_NAME)\n",
    "\n",
    "# chatbot_funnel_blobs = []\n",
    "chatbot_data_blobs = []\n",
    "\n",
    "checkout_data_blobs = []\n",
    "\n",
    "# blobs = bucket.list_blobs(prefix=get_funnel_prefix_in_refined_zone(CURRENT_DATE, ORIGIN, \"chatbot\"))\n",
    "# chatbot_funnel_blobs = [f\"gs://{BUCKET_NAME}/{blob.name}\" for blob in blobs if blob.name.endswith(\".parquet/\")]\n",
    "\n",
    "blobs = bucket.list_blobs(prefix=get_data_prefix_in_refined_zone(CURRENT_DATE, ORIGIN, \"chatbot\"))\n",
    "chatbot_data_blobs = [f\"gs://{BUCKET_NAME}/{blob.name}\" for blob in blobs if blob.name.endswith(\".parquet/\")]\n",
    "\n",
    "blobs = bucket.list_blobs(prefix=get_data_prefix_in_refined_zone(CURRENT_DATE, ORIGIN, \"checkout\"))\n",
    "checkout_data_blobs = [f\"gs://{BUCKET_NAME}/{blob.name}\" for blob in blobs if blob.name.endswith(\".parquet/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "JoinOnField = namedtuple('JoinOnField', ['field_name', 'isNullable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_on_bigquery(df: DataFrame, table_name: str, fields: list, joinOn: List[JoinOnField]):\n",
    "  tmp_table_name = get_tmp_table_name(table_name)\n",
    "  full_table_name = get_full_table_name(table_name)\n",
    "  table = None\n",
    "\n",
    "  try:\n",
    "    table = bqClient.get_table(full_table_name)\n",
    "  except:\n",
    "    table = None\n",
    "\n",
    "  if table is None:\n",
    "    df.write.format('bigquery')\\\n",
    "      .option(\"temporaryGcsBucket\", BUCKET_NAME)\\\n",
    "      .option(\"table\", full_table_name)\\\n",
    "      .option(\"parentProject\", PARENT_PROJECT)\\\n",
    "      .mode(\"overwrite\")\\\n",
    "      .save()\n",
    "  else:\n",
    "    bqClient.query(f\"DROP TABLE IF EXISTS {tmp_table_name}\")\n",
    "\n",
    "    df.write.format(\"bigquery\")\\\n",
    "      .option(\"temporaryGcsBucket\", BUCKET_NAME)\\\n",
    "      .option(\"table\", tmp_table_name)\\\n",
    "      .option(\"parentProject\", PARENT_PROJECT)\\\n",
    "      .mode(\"append\")\\\n",
    "      .save()\n",
    "\n",
    "    select = \", \".join(fields)\n",
    "    joinCondition = \" AND \".join([\n",
    "      f\"(COALESCE(target.{field.field_name}, \\\"\\\") = COALESCE(origin.{field.field_name}, \\\"\\\"))\" if field.isNullable \n",
    "      else f\"target.{field.field_name} = origin.{field.field_name}\" \n",
    "      for field in joinOn\n",
    "    ])\n",
    "    update = \", \".join([f\"target.{field} = origin.{field}\" for field in fields])\n",
    "    insertFields = \", \".join(fields)\n",
    "    insertValues = \", \".join([f\"origin.{field}\" for field in fields])\n",
    "\n",
    "    query = f\"\"\"\n",
    "        MERGE INTO {full_table_name} AS target\n",
    "        USING (SELECT {select} FROM {tmp_table_name}) AS origin\n",
    "        ON {joinCondition}\n",
    "        WHEN MATCHED THEN \n",
    "          UPDATE SET {update}\n",
    "        WHEN NOT MATCHED THEN\n",
    "          INSERT ({insertFields})\n",
    "          VALUES ({insertValues})\n",
    "      \"\"\"\n",
    "    bqClient.query(query).result()\n",
    "    print(\"Saved on BigQuery: \", table_name)\n",
    "    bqClient.query(f\"DROP TABLE {tmp_table_name}\")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_blob in chatbot_data_blobs:\n",
    "  df_data = spark.read.parquet(data_blob)\n",
    "  save_on_bigquery(df_data.dropDuplicates([\"order_id\", \"product_type\", \"project_id\"]), \"chatbot_history\", list(df_data.columns), [\n",
    "    JoinOnField(\"order_id\", False),\n",
    "    JoinOnField(\"product_type\", False),\n",
    "    JoinOnField(\"project_id\", False),\n",
    "  ])\n",
    "\n",
    "# for funnel_blob in chatbot_funnel_blobs:\n",
    "#   funnel_data = spark.read.parquet(funnel_blob)\n",
    "#   save_on_bigquery(funnel_data.dropDuplicates([\"order_id\", \"product_type\", \"project_id\", \"step\"]), \"chatbot_funnel\", list(funnel_data.columns), [\n",
    "#     JoinOnField(\"order_id\", False),\n",
    "#     JoinOnField(\"product_type\", False),\n",
    "#     JoinOnField(\"project_id\", False),\n",
    "#     JoinOnField(\"step\", False),\n",
    "#   ])\n",
    "\n",
    "for data_blob in checkout_data_blobs:\n",
    "  df_data = spark.read.parquet(data_blob)\n",
    "  save_on_bigquery(df_data.dropDuplicates([\"order_id\", \"product_type\", \"project_id\"]), \"checkout_history\", list(df_data.columns), [\n",
    "    JoinOnField(\"order_id\", False),\n",
    "    JoinOnField(\"product_type\", False),\n",
    "    JoinOnField(\"project_id\", False),\n",
    "  ])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
