{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, BooleanType\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from google.cloud import storage, bigquery\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "CURRENT_DATE_ARG = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_PROJECT = \"cloud-macro-blip\"\n",
    "BUCKET_NAME = \"vivo-api-manager-gcs-sp\"\n",
    "ORIGIN = \"vivo-b2b-movel\"\n",
    "CURRENT_DATE = datetime.strptime(CURRENT_DATE_ARG, '%Y-%m-%dT%H:%M:%S') if CURRENT_DATE_ARG is not None else datetime.today() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRAZILIAN_TIMEDIFF = timedelta(hours=3)\n",
    "CURRENT_DATE = CURRENT_DATE - BRAZILIAN_TIMEDIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_midnight_hour(hour):\n",
    "  return hour >= 0 and hour <= 4\n",
    "\n",
    "if is_midnight_hour(CURRENT_DATE.hour):\n",
    "  CURRENT_DATE = CURRENT_DATE - timedelta(days=1)\n",
    "  CURRENT_DATE = CURRENT_DATE.replace(hour=23, minute=59, second=59)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.storage.client.Client object at 0x7fb3b05df090>\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder.appName(\"curated_to_refined\")\\\n",
    "#   .config('parentProject', PARENT_PROJECT)\\\n",
    "#   .config(\"spark.sql.caseSensitive\", \"True\")\\\n",
    "#   .config('spark.sql.session.timeZone', 'America/Sao_Paulo')\\\n",
    "#   .config(\"spark.jars\", \"gs://spark-lib/bigquery/spark-3.3-bigquery-0.34.0.jar\")\\\n",
    "#   .getOrCreate()\n",
    "\n",
    "# gcsClient = storage.Client()\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/jovyan/work/macro_blip_b2b_movel/cloud-macro-blip-cb70a4da44e6.json\"\n",
    "\n",
    "# Criar a sess√£o Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ler Parquet do GCS\") \\\n",
    "    .config(\"spark.jars\", \n",
    "            \"/usr/local/spark/jars/gcs-connector-hadoop3-latest.jar,\"\n",
    "            \"/usr/local/spark/jars/hadoop-common-3.3.6.jar,\"\n",
    "            \"/usr/local/spark/jars/hadoop-client-api-3.3.4.jar,\"\n",
    "            \"/usr/local/spark/jars/hadoop-client-runtime-3.3.4.jar\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]) \\\n",
    "    .config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "gcsClient = storage.Client()\n",
    "print(gcsClient)\n",
    "\n",
    "bqClient = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path_in_refined_zone(datetime: datetime, origin: str, project_type: str, table_name: str):\n",
    "  return f\"gs://{BUCKET_NAME}/{origin}/refined-zone/data/{project_type}/{datetime.strftime('%Y%m')}/{table_name}.parquet\"\n",
    "\n",
    "# def get_funnel_path_in_refined_zone(datetime: datetime, origin: str, project_type: str, table_name: str):\n",
    "#   return f\"gs://{BUCKET_NAME}/{origin}/refined-zone/funnel/{project_type}/{datetime.strftime('%Y%m')}/{table_name}.parquet\"\n",
    "\n",
    "# def get_funnel_prefix_in_curated_zone(datetime: datetime, origin: str):\n",
    "#   return f\"{origin}/curated-zone/funnel/{datetime.strftime('%Y%m')}\"\n",
    "\n",
    "def get_data_prefix_in_curated_zone(datetime: datetime, origin: str):\n",
    "  return f\"{origin}/curated-zone/data/{datetime.strftime('%Y%m')}\"\n",
    "\n",
    "def reduce_dataframe(dfs: list):\n",
    "  if len(dfs) == 0:\n",
    "    return spark.createDataFrame([], StringType())\n",
    "  elif len(dfs) == 1:\n",
    "    return dfs[0]\n",
    "  else:\n",
    "    return reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = gcsClient.get_bucket(BUCKET_NAME)\n",
    "\n",
    "funnel_blobs = []\n",
    "data_blobs = []\n",
    "\n",
    "# blobs = bucket.list_blobs(prefix=get_funnel_prefix_in_curated_zone(CURRENT_DATE, ORIGIN))\n",
    "# funnel_blobs = [f\"gs://{BUCKET_NAME}/{blob.name}\" for blob in blobs if blob.name.endswith(\".parquet/\")]\n",
    "\n",
    "blobs = bucket.list_blobs(prefix=get_data_prefix_in_curated_zone(CURRENT_DATE, ORIGIN))\n",
    "data_blobs = [f\"gs://{BUCKET_NAME}/{blob.name}\" for blob in blobs if blob.name.endswith(\".parquet/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funnel_columns = [\n",
    "#   \"order_id\",\n",
    "#   \"product_type\",\n",
    "#   \"project_id\",\n",
    "#   \"processed_at\",\n",
    "#   \"created_in\",\n",
    "#   \"step\",\n",
    "#   \"end_date_process\",\n",
    "#   \"init_date_process\",\n",
    "#   \"order_ref\",\n",
    "#   \"project_type\",\n",
    "#   \"sale_type\",\n",
    "#   \"send_crm\",\n",
    "#   \"status\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funnel_dfs = []\n",
    "# for funnel_blob in funnel_blobs:\n",
    "#   funnel_df = spark.read.parquet(funnel_blob)\n",
    "#   funnel_dfs.append(funnel_df)\n",
    "\n",
    "# final_funnel_df = reduce_dataframe(funnel_dfs)\n",
    "# funnel_df_grouped = final_funnel_df\\\n",
    "#   .groupBy(\"order_id\", \"product_type\", \"project_id\")\\\n",
    "#   .agg(F.max(\"processed_at\").alias(\"processed_at\"))\\\n",
    "#   .orderBy(\"processed_at\")\n",
    "\n",
    "# final_funnel_df = funnel_df_grouped.join(final_funnel_df, funnel_df_grouped.columns, \"inner\")\n",
    "# final_funnel_df = final_funnel_df\\\n",
    "#   .withColumn(\"processed_at\", F.date_format(F.from_utc_timestamp(F.col(\"processed_at\"), \"UTC\"), \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "#   .withColumn(\"created_in\", F.date_format(F.from_utc_timestamp(F.col(\"created_in\"), \"America/Sao_Paulo\"), \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "#   .withColumn(\"end_date_process\", F.date_format(F.from_utc_timestamp(F.col(\"end_date_process\"), \"America/Sao_Paulo\"), \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "#   .withColumn(\"init_date_process\", F.date_format(F.from_utc_timestamp(F.col(\"init_date_process\"), \"America/Sao_Paulo\"), \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "#   .select(funnel_columns)\n",
    "\n",
    "# final_funnel_df.where(F.col(\"project_type\") == \"Chatbot\").write.mode(\"overwrite\").parquet(get_funnel_path_in_refined_zone(CURRENT_DATE, ORIGIN, \"chatbot\", \"chatbot_funnel\"))\n",
    "# # final_funnel_df.where(F.col(\"project_type\") == \"Chatbot\").write.mode(\"overwrite\").parquet(get_funnel_path_in_refined_zone(CURRENT_DATE, ORIGIN, \"chatbot\", \"chatbot_funnel\"))\n",
    "# final_funnel_df.where(F.col(\"project_type\") == \"Checkout\").write.mode(\"overwrite\").parquet(get_funnel_path_in_refined_zone(CURRENT_DATE, ORIGIN, \"checkout\", \"checkout_funnel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_history_columns = [\n",
    "  'order_id',\n",
    "  'crm_order_id',\n",
    "  'end_date_process',\n",
    "  'init_date_process',\n",
    "  'order_ref',\n",
    "  'product_type',\n",
    "  'project_id',\n",
    "  'project_type',\n",
    "  'send_crm',\n",
    "  'status',\n",
    "  'processed_at',\n",
    "  'sku',\n",
    "  'telefone',\n",
    "  'telefone_secundario',\n",
    "  'utm_dados_adicionais',\n",
    "  'utm_medium',\n",
    "  'utm_source',\n",
    "  'utm_campaign',\n",
    "  'utm_referrer',\n",
    "  'utm_term',\n",
    "  'sale_type',\n",
    "]\n",
    "\n",
    "chatbot_history_columns = [\n",
    "  'email',\n",
    "  'cep',\n",
    "  'logradouro',\n",
    "  'num_imovel',\n",
    "  'bairro',\n",
    "  'estado',\n",
    "  'cidade',\n",
    "  'complemento',\n",
    "  'ponto_referencia',\n",
    "  'cpf',\n",
    "  'empresa_bairro',\n",
    "  'empresa_cep',\n",
    "  'empresa_cidade',\n",
    "  'empresa_complemento',\n",
    "  'empresa_estado',\n",
    "  'empresa_logradouro',\n",
    "  'empresa_num_imovel',\n",
    "  'cnpj',\n",
    "  'razao_social',\n",
    "  'nome_fantasia',\n",
    "  'socios',\n",
    "  'empresa_telefone',\n",
    "  'nome',\n",
    "  'data_nascimento',\n",
    "  'nome_mae',\n",
    "  'contrato_checked',\n",
    "  'termo',\n",
    "  'linhas_moveis',\n",
    "  'cliente_base',\n",
    "  'codinome',\n",
    "  'licencas_office',\n",
    "  'projeto',\n",
    "]\n",
    "\n",
    "checkout_history_columns = ['autenticacao_data', 'autenticacao_id', 'autenticacao_token']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dfs = []\n",
    "for data_blob in data_blobs:\n",
    "  data_df = spark.read.parquet(data_blob)\n",
    "  data_dfs.append(data_df)\n",
    "\n",
    "data_df = reduce_dataframe(data_dfs)\n",
    "data_df_grouped = data_df\\\n",
    "  .groupBy(\"order_id\", \"product_type\", \"project_id\")\\\n",
    "  .agg(F.max(\"processed_at\").alias(\"processed_at\"))\\\n",
    "  .orderBy(\"processed_at\")\\\n",
    "  .select(\"*\")\n",
    "\n",
    "final_data_df = data_df_grouped.join(data_df, data_df_grouped.columns, \"inner\")\n",
    "final_data_df = final_data_df\\\n",
    "  .withColumn(\"processed_at\", F.date_format(F.from_utc_timestamp(F.col(\"processed_at\"), \"UTC\"), \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "  .withColumn(\"end_date_process\", F.date_format(F.from_utc_timestamp(F.col(\"end_date_process\"), \"America/Sao_Paulo\"), \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "  .withColumn(\"init_date_process\", F.date_format(F.from_utc_timestamp(F.col(\"init_date_process\"), \"America/Sao_Paulo\"), \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "  .select(\"*\")\n",
    "\n",
    "chatbot_history = final_data_df.where(F.col(\"project_type\") == \"Chatbot\").select(*[base_history_columns + chatbot_history_columns])\n",
    "checkout_history = final_data_df.where(F.col(\"project_type\") == \"Checkout\").select(*[base_history_columns])\n",
    "\n",
    "chatbot_history.write.mode(\"overwrite\").parquet(get_data_path_in_refined_zone(CURRENT_DATE, ORIGIN, \"chatbot\", \"chatbot_history\"))\n",
    "checkout_history.write.mode(\"overwrite\").parquet(get_data_path_in_refined_zone(CURRENT_DATE, ORIGIN, \"checkout\", \"checkout_history\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
