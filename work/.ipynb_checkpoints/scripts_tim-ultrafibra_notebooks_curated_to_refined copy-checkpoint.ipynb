{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb20d863",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T11:15:41.783057Z",
     "iopub.status.busy": "2024-10-18T11:15:41.782803Z",
     "iopub.status.idle": "2024-10-18T11:15:43.251994Z",
     "shell.execute_reply": "2024-10-18T11:15:43.251267Z"
    },
    "papermill": {
     "duration": 1.474381,
     "end_time": "2024-10-18T11:15:43.253772",
     "exception": false,
     "start_time": "2024-10-18T11:15:41.779391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from google.cloud import storage, bigquery\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9623249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T11:15:43.258434Z",
     "iopub.status.busy": "2024-10-18T11:15:43.258063Z",
     "iopub.status.idle": "2024-10-18T11:15:43.261013Z",
     "shell.execute_reply": "2024-10-18T11:15:43.260536Z"
    },
    "papermill": {
     "duration": 0.006533,
     "end_time": "2024-10-18T11:15:43.262212",
     "exception": false,
     "start_time": "2024-10-18T11:15:43.255679",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "CURRENT_DATE_ARG = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376227ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T11:15:43.266567Z",
     "iopub.status.busy": "2024-10-18T11:15:43.265991Z",
     "iopub.status.idle": "2024-10-18T11:15:43.269461Z",
     "shell.execute_reply": "2024-10-18T11:15:43.268957Z"
    },
    "papermill": {
     "duration": 0.006978,
     "end_time": "2024-10-18T11:15:43.270748",
     "exception": false,
     "start_time": "2024-10-18T11:15:43.263770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"tim-ultrafibra-gcs-sp\"\n",
    "PARENT_PROJECT = \"cloud-macro-tim\"\n",
    "CURRENT_DATE = datetime.strptime(CURRENT_DATE_ARG, '%Y-%m-%dT%H:%M:%S') if CURRENT_DATE_ARG is not None else datetime.today() \n",
    "ORIGINS = [\"bot_tim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29aeefc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T11:15:43.274907Z",
     "iopub.status.busy": "2024-10-18T11:15:43.274519Z",
     "iopub.status.idle": "2024-10-18T11:15:43.277575Z",
     "shell.execute_reply": "2024-10-18T11:15:43.276993Z"
    },
    "papermill": {
     "duration": 0.006595,
     "end_time": "2024-10-18T11:15:43.278902",
     "exception": false,
     "start_time": "2024-10-18T11:15:43.272307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BRAZILIAN_TIMEDIFF = timedelta(hours=3)\n",
    "CURRENT_DATE = CURRENT_DATE - BRAZILIAN_TIMEDIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80ca8ac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T11:15:43.283152Z",
     "iopub.status.busy": "2024-10-18T11:15:43.282599Z",
     "iopub.status.idle": "2024-10-18T11:15:43.286277Z",
     "shell.execute_reply": "2024-10-18T11:15:43.285734Z"
    },
    "papermill": {
     "duration": 0.006945,
     "end_time": "2024-10-18T11:15:43.287373",
     "exception": false,
     "start_time": "2024-10-18T11:15:43.280428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-18 14:04:28.242729\n"
     ]
    }
   ],
   "source": [
    "def is_midnight_hour(hour):\n",
    "  return hour >= 0 and hour <= 4\n",
    "\n",
    "if is_midnight_hour(CURRENT_DATE.hour):\n",
    "  CURRENT_DATE = CURRENT_DATE - timedelta(days=1)\n",
    "  CURRENT_DATE = CURRENT_DATE.replace(hour=23, minute=59, second=59)\n",
    "  print(CURRENT_DATE)\n",
    "print(CURRENT_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56aedad5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T11:15:43.291581Z",
     "iopub.status.busy": "2024-10-18T11:15:43.291013Z",
     "iopub.status.idle": "2024-10-18T11:15:50.909055Z",
     "shell.execute_reply": "2024-10-18T11:15:50.908169Z"
    },
    "papermill": {
     "duration": 7.622247,
     "end_time": "2024-10-18T11:15:50.911095",
     "exception": false,
     "start_time": "2024-10-18T11:15:43.288848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtim_ultrafibra_curated_to_refined\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparentProject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPARENT_PROJECT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.caseSensitive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark.sql.session.timeZone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAmerica/Sao_Paulo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://spark-lib/bigquery/spark-3.3-bigquery-0.34.0.jar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 6\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m gcsClient \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mClient()\n\u001b[0;32m      9\u001b[0m bqClient \u001b[38;5;241m=\u001b[39m bigquery\u001b[38;5;241m.\u001b[39mClient()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"tim_ultrafibra_curated_to_refined\")\\\n",
    "  .config('parentProject', PARENT_PROJECT)\\\n",
    "  .config(\"spark.sql.caseSensitive\", \"True\")\\\n",
    "  .config('spark.sql.session.timeZone', 'America/Sao_Paulo')\\\n",
    "  .config(\"spark.jars\", \"gs://spark-lib/bigquery/spark-3.3-bigquery-0.34.0.jar\")\\\n",
    "  .getOrCreate()\n",
    "\n",
    "gcsClient = storage.Client()\n",
    "bqClient = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa3109bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T11:15:50.918341Z",
     "iopub.status.busy": "2024-10-18T11:15:50.917707Z",
     "iopub.status.idle": "2024-10-18T11:15:50.921949Z",
     "shell.execute_reply": "2024-10-18T11:15:50.921393Z"
    },
    "papermill": {
     "duration": 0.009606,
     "end_time": "2024-10-18T11:15:50.923567",
     "exception": false,
     "start_time": "2024-10-18T11:15:50.913961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_path_in_curated_zone(datetime: datetime, origin: str):\n",
    "  return f\"gs://{BUCKET_NAME}/curated-zone/{origin}/{datetime.strftime('%Y%m')}/{datetime.strftime('%Y%m%d')}.parquet\"\n",
    "\n",
    "def get_path_in_refined_zone(datetime: datetime, origin: str, table_name: str):\n",
    "  return f\"gs://{BUCKET_NAME}/refined-zone/{origin}/{datetime.strftime('%Y%m')}/{table_name}.parquet\"\n",
    "\n",
    "def get_prefix_in_curated(datetime: datetime, origin: str):\n",
    "  return f\"curated-zone/{origin}/{datetime.strftime('%Y%m')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345097c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T11:15:50.930048Z",
     "iopub.status.busy": "2024-10-18T11:15:50.929803Z",
     "iopub.status.idle": "2024-10-18T11:15:50.934436Z",
     "shell.execute_reply": "2024-10-18T11:15:50.933688Z"
    },
    "papermill": {
     "duration": 0.010042,
     "end_time": "2024-10-18T11:15:50.936352",
     "exception": false,
     "start_time": "2024-10-18T11:15:50.926310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_dataframe(dfs: list):\n",
    "  if len(dfs) == 0:\n",
    "    return spark.createDataFrame([], StringType())\n",
    "  elif len(dfs) == 1:\n",
    "    return dfs[0]\n",
    "  else:\n",
    "    return reduce(DataFrame.unionByName, dfs)\n",
    "\n",
    "def save_dataframe(df: DataFrame, origin: str, table_name: str):\n",
    "  df.write.format(\"parquet\").mode(\"overwrite\").save(get_path_in_refined_zone(CURRENT_DATE,origin, table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58cfe3bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T11:15:50.943228Z",
     "iopub.status.busy": "2024-10-18T11:15:50.942819Z",
     "iopub.status.idle": "2024-10-18T11:15:51.207742Z",
     "shell.execute_reply": "2024-10-18T11:15:51.206993Z"
    },
    "papermill": {
     "duration": 0.270288,
     "end_time": "2024-10-18T11:15:51.209453",
     "exception": false,
     "start_time": "2024-10-18T11:15:50.939165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "blobs_per_origin = {}\n",
    "\n",
    "for origin_name in ORIGINS:\n",
    "  blobs = gcsClient.list_blobs(BUCKET_NAME, prefix=get_prefix_in_curated(CURRENT_DATE, origin_name))\n",
    "  blobs_per_origin[origin_name] = [f\"gs://{BUCKET_NAME}/{blob.name}\" for blob in blobs if blob.name.endswith(\".parquet/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aeb784a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T11:15:51.214357Z",
     "iopub.status.busy": "2024-10-18T11:15:51.214117Z",
     "iopub.status.idle": "2024-10-18T11:17:02.999382Z",
     "shell.execute_reply": "2024-10-18T11:17:02.998569Z"
    },
    "papermill": {
     "duration": 71.790556,
     "end_time": "2024-10-18T11:17:03.001953",
     "exception": false,
     "start_time": "2024-10-18T11:15:51.211397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for origin in ORIGINS:\n",
    "  dfs_leads = []\n",
    "  dfs_lead_addresses = []\n",
    "  dfs_orders = []\n",
    "  dfs_histories = []\n",
    "\n",
    "  for path in blobs_per_origin[origin]:\n",
    "    df = spark.read.parquet(path)\n",
    "\n",
    "    step_columns = [col for col in df.columns if col.startswith(\"etapa_\")]\n",
    "\n",
    "    df = df.withColumn(\"last_message_date\",\n",
    "      F.col(\"last_message_date\").cast(StringType()).substr(0, 19)\n",
    "    )\n",
    "\n",
    "    df_leads = df.dropDuplicates([\"identity\", \"cpf\"])\\\n",
    "      .select(\"identity\", \"name\", \"cpf\", \"phone_number\")\\\n",
    "      .distinct()\n",
    "\n",
    "    df_order = df.dropDuplicates([\"identity\", \"last_message_date\"])\\\n",
    "      .where(F.col(\"etapa_finalizacao\") == True)\\\n",
    "      .select([\n",
    "        \"identity\",\n",
    "        \"last_message_date\",\n",
    "        \"identificador_pedido\",\n",
    "        \"plan_name\",\n",
    "        \"plan_description\",\n",
    "        \"price\"\n",
    "      ])\\\n",
    "      .distinct()\n",
    "\n",
    "    df_history = df.withColumn(\"date\", F.to_date(F.col(\"last_message_date\")))\\\n",
    "      .withColumn(\"hour\", F.date_format(F.col(\"last_message_date\"), 'HH'))\\\n",
    "      .withColumn(\"identity_date\", F.concat(F.col(\"identity\"), F.lit(\"_\"), F.col(\"date\")))\\\n",
    "      .select(\"identity\", \"date\", \"hour\", \"tipo\", \"identificador_pedido\", \"num_bot\",\n",
    "      'utm_term', 'utm_campaign', \"identity_date\", \"primeira_mensagem\", \"last_message_date\", \"step_abandono\", *step_columns)\\\n",
    "      .dropDuplicates([\"identity\", \"date\", \"hour\"])\\\n",
    "      .distinct()\\\n",
    "      .sort(F.col(\"last_message_date\").asc())\n",
    "\n",
    "    dfs_leads.append(df_leads)\n",
    "    dfs_orders.append(df_order)\n",
    "    dfs_histories.append(df_history)\n",
    "    pass\n",
    "\n",
    "  df_final_leads = reduce_dataframe(dfs_leads)\n",
    "  df_final_lead_addresses = reduce_dataframe(dfs_lead_addresses)\n",
    "  df_final_order = reduce_dataframe(dfs_orders)\n",
    "  df_final_history = reduce_dataframe(dfs_histories)\n",
    "\n",
    "  if df_final_leads is not None:\n",
    "    save_dataframe(df=df_final_leads, origin=origin, table_name=\"leads\")\n",
    "\n",
    "  if df_final_lead_addresses is not None:\n",
    "    save_dataframe(df=df_final_lead_addresses, origin=origin, table_name=\"lead_addresses\")\n",
    "\n",
    "  if df_final_order is not None:\n",
    "    save_dataframe(df=df_final_order, origin=origin, table_name=\"orders\")\n",
    "\n",
    "  if df_final_history is not None:\n",
    "    save_dataframe(df=df_final_history, origin=origin, table_name=\"history_leads\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 82.744749,
   "end_time": "2024-10-18T11:17:03.534620",
   "environment_variables": {},
   "exception": null,
   "input_path": "input_curated.ipynb",
   "output_path": "output_curated.ipynb",
   "parameters": {},
   "start_time": "2024-10-18T11:15:40.789871",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
